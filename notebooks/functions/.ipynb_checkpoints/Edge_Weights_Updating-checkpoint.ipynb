{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0659989-d61d-41fe-a003-ccd9fc1d32ed",
   "metadata": {},
   "source": [
    "# Edge Weights Updating\n",
    "<b>Date:</b> October 21, 2023 \\\n",
    "<b>Author:</b> Dimitris Lymperopoulos \\\n",
    "<b>Description:</b> A notebook containing necessary functions for fine-tuning the bipartite graph for edits generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1321ef-6f3b-42e0-94cd-879f0e9d8129",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "* Create parallelization for fluency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1dc1c0-7436-4858-90fb-ac46e2a99c25",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf4f91-ecbb-4c70-beaf-333adf4c1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install networkx\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ed90d-2860-44e9-a6a9-7b250119a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install polyjuice_nlp\n",
    "# !pip install torch\n",
    "# !pip install evaluate\n",
    "# !pip install bert_score\n",
    "# !python -m spacy download en_core_web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207c67a-41c8-4b59-8e99-13af2cc75c7e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a592d877-5b2e-4a19-abcf-c040232dfd4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Metric-related imports\n",
    "import torch\n",
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from evaluate import load\n",
    "from joblib import Parallel, delayed\n",
    "from pylev import levenshtein as lev_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20e5d0b-61b7-47ae-8f99-1e98030f0c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimli\\anaconda3\\envs\\nlp_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%run functions/GPT2_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da0bdef-b3f2-43d1-aa10-07dbe54b6c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run functions/graph_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b8eadd-dc09-40b7-982c-1e15e4a293fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f961da7b-c435-40bb-85c2-ff7c137a7b7a",
   "metadata": {},
   "source": [
    "## Edge Updating Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b310acda-a6d3-48b3-842a-189bc0b58946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_edges(edges, substitutions, lr, baseline_metric_value, current_metric_value):\n",
    "    \"\"\"\n",
    "    A function that takes as input a list of weighted edges along with other parameters, and uses\n",
    "    these parameters to update the edge weights.\n",
    "\n",
    "    :param edges: an iterable containing weighted edges as tuples\n",
    "    :param substitutions: a dictionary with edges as keys, and their substitution occurence as values\n",
    "    :param lr: float value, representing the learning rate for the weight updating\n",
    "    :param baseline_metric_value: a float, representing the baseline evaluation metric value\n",
    "    :param current_metric_value: a float, representing the current evaluation metric value\n",
    "    :returns: a list of tuples, where each tuple represents an updated weighted edge\n",
    "    \"\"\"\n",
    "    \n",
    "    updated_edges = list()\n",
    "    for (u, v, w) in edges:\n",
    "        try:\n",
    "            # get substitution occurences for each edge\n",
    "            edge_subs = substitutions[(u,v)] \n",
    "            # updating formula\n",
    "            new_w = w - lr * (baseline_metric_value - current_metric_value) / edge_subs   # - for minimizing, + for maximizing\n",
    "            # add the updated edge to the list\n",
    "            updated_edges.append((u, v, new_w))\n",
    "        except KeyError:\n",
    "            print(\"Something went wrong during updating of edges' weights\")\n",
    "    \n",
    "    return updated_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd687acd-4f4f-4bff-acab-ce0b5b9db89a",
   "metadata": {},
   "source": [
    "## Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab34fc3b-6c1d-468d-b323-11829b0895dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fluency(data, counter_data):\n",
    "    \"\"\"\n",
    "    A function that takes as input the original and the counter data and returns the average fluency\n",
    "    between the sentence pairs\n",
    "\n",
    "    :param data: dataframe containing one column with the original data\n",
    "    :param counter_data: dataframe containing one column with the counter data\n",
    "    :returns: float value representing the average fluency\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract sentences and counter-sentences from the data and check that they are of the same length\n",
    "    sentences = [elem[0] for elem in data.values.tolist()]\n",
    "    counter_sentences= [elem[0] for elem in counter_data.values.tolist()]\n",
    "\n",
    "    assert len(sentences) == len(counter_sentences)\n",
    "    \n",
    "    # compute average fluency\n",
    "    cuda = torch.cuda.is_available()\n",
    "    model, tokenizer = model_init(cuda=cuda)\n",
    "    avg_fluency, counter = 0, 0\n",
    "    for x in zip(sentences, counter_sentences):\n",
    "        if len(x[0]) <= 1024 and len(x[1]) <= 1024:\n",
    "            avg_fluency += abs(sent_scoring(model, tokenizer, x[0], cuda=cuda)[0] - sent_scoring(model, tokenizer, x[1], cuda=cuda)[0])\n",
    "            counter += 1\n",
    "\n",
    "        # except IndexError:    # when sentence is too large, it cannot fit into the model, thus causing IndexError\n",
    "        #     continue\n",
    "        # except RuntimeError:\n",
    "        #     continue\n",
    "    print(counter)\n",
    "    return avg_fluency / counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1400ae-8731-45af-9887-7afd3f846871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     'sents': [\n",
    "#         'A great man was standing in a tall and magnificent hill, gazing upon the sad and destructive army',\n",
    "#         'The clever boy was wondering when the fat dog would return with the big stick',\n",
    "#         'A small town was standing next to the large river and the tall building'\n",
    "#     ]\n",
    "# })\n",
    "\n",
    "# counter_df = pd.DataFrame({\n",
    "#     'sents': [\n",
    "#         'A small man was standing in a large and magnificent hill, gazing upon the happy and destructive army',\n",
    "#         'The dumb boy was wondering when the slim dog would return with the tinny stick',\n",
    "#         'A big town was standing next to the tall river and the large building'\n",
    "#     ]\n",
    "    \n",
    "# })\n",
    "\n",
    "# get_fluency(df, counter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05021dd6-6e0f-4cf8-af9e-819681c171f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closeness(data, counter_data):\n",
    "    \"\"\"\n",
    "    A function that takes as input the original and the counter data and returns the average levenshtein\n",
    "    distance as a measure of closeness between the sentence pairs\n",
    "\n",
    "    :param data: dataframe containing one column with the original data\n",
    "    :param counter_data: dataframe containing one column with the counter data\n",
    "    :returns: float value representing the average levenshtein distance\n",
    "    \"\"\"\n",
    "        \n",
    "    # extract sentences and counter-sentences from the data and check that they are of the same length\n",
    "    sentences = [elem[0] for elem in data.values.tolist()]\n",
    "    counter_sentences= [elem[0] for elem in counter_data.values.tolist()]\n",
    "\n",
    "    assert len(sentences) == len(counter_sentences)\n",
    "\n",
    "    # compute average levenshtein distance as a measurement of closeness\n",
    "    avg_lev = sum(Parallel(n_jobs=-1)(delayed(lev_dist)(x[0], x[1]) for x in zip(sentences, counter_sentences))) / len(sentences)\n",
    "\n",
    "    return avg_lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ee5381-f0d6-4510-bc86-96a5a5527c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bertscore(data, counter_data):\n",
    "    \"\"\"\n",
    "    A function that takes as input the original and the counter data and returns the average bertscore\n",
    "    between the sentence pairs\n",
    "\n",
    "    :param data: dataframe containing one column with the original data\n",
    "    :param counter_data: dataframe containing one column with the counter data\n",
    "    :returns: float value representing the average bertscore\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract sentences and counter-sentences from the data and check that they are of the same length\n",
    "    sentences = [elem[0] for elem in data.values.tolist()]\n",
    "    counter_sentences= [elem[0] for elem in counter_data.values.tolist()]\n",
    "\n",
    "    assert len(sentences) == len(counter_sentences)\n",
    "\n",
    "    # compute average bertscore\n",
    "    avg_bertscore = sum(bertscore.compute(predictions=counter_sentences, references=sentences, model_type=\"distilbert-base-uncased\", nthreads=20)['f1']) / len(sentences)\n",
    "\n",
    "    return avg_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "879b9afc-e1e1-4ce2-9e14-19e13f8ccaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flip_rate(original_p, counter_p):\n",
    "    \"\"\"\n",
    "    A function that takes as input the original predictions and the new ones, and returns the  \n",
    "    flip-rate as a percentage.\n",
    "\n",
    "    :param original_p: list containing the predictions for the original data\n",
    "    :param counter_p: dataframe containing the predictions for the counter data\n",
    "    :returns: dictionary containing model-related metrics\n",
    "    \"\"\"\n",
    "                     \n",
    "    # check that predictions and counter_predictions are of the same length\n",
    "    assert len(original_p) == len(counter_p)\n",
    "    \n",
    "    # compute flip_rate\n",
    "    flip_rate_percent = sum(Parallel(n_jobs=-1)(delayed(lambda x: x[0] != x[1])(x) for x in zip(original_p, counter_p))) / len(original_p)\n",
    "\n",
    "    return flip_rate_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911ab48-673f-403e-9d98-1275956ad092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fluency_bertscore(data, counter_data):\n",
    "    fl, bs = get_fluency(data, counter_data),  1 - get_bertscore(data, counter_data)\n",
    "\n",
    "    return 2 * fl * bs / (fl + bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e54ccdc-b68b-481f-b972-cd9d5b73ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_model_agnostic_metrics(data, counter_data):\n",
    "#     \"\"\"\n",
    "#     A function that takes as input the original and the counter data and returns a dictionary with \n",
    "#     model-agnostic metrics such as closeness and fluency.\n",
    "\n",
    "#     :param data: dataframe containing one column with the original data\n",
    "#     :param counter_data: dataframe containing one column with the counter data\n",
    "#     :returns: dictionary containing model-agnostic metrics\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # extract sentences and counter-sentences from the data and check that they are of the same length\n",
    "#     sentences = [elem[0] for elem in data.values.tolist()]\n",
    "#     counter_sentences= [elem[0] for elem in counter_data.values.tolist()]\n",
    "\n",
    "#     assert len(sentences) == len(counter_sentences)\n",
    "\n",
    "#     sent_length = len(sentences)\n",
    "\n",
    "#     # compute average levenshtein distance as a measurement of closeness\n",
    "#     #avg_lev = sum(list(map(lambda x: lev_dist(x[0], x[1]), zip(sentences, counter_sentences)))) / sent_length\n",
    "\n",
    "#     # compute average fluency\n",
    "#     model, tokenizer = model_init()\n",
    "#     #avg_fluency = sum(list(map(lambda x: sent_scoring(model, tokenizer, x)[0], counter_sentences))) / sent_length\n",
    "#     avg_fluency = sum(\n",
    "#         list(map(lambda x: abs(sent_scoring(model, tokenizer, x[0])[0] - sent_scoring(model, tokenizer, x[1])[0]), zip(sentences, counter_sentences)))\n",
    "#     ) / len(sentences)\n",
    "\n",
    "#     # compute average bertscore\n",
    "#    # avg_bertscore = 1 - sum(bertscore.compute(predictions=counter_sentences, references=sentences, model_type=\"distilbert-base-uncased\")['f1']) / sent_length\n",
    "    \n",
    "#     # create metrics dictionary\n",
    "#     metrics = {\n",
    "#         #'levenshtein': avg_lev,     # we want this to be as low as possible\n",
    "#         'fluency': avg_fluency,     # we want this to be as low as possible (it is the difference |original_fluency - counter_fluency|)\n",
    "#         #'bertscore': avg_bertscore  #  we want this to be as low as possible\n",
    "#     }\n",
    "\n",
    "#     return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "702e09a8-361a-4d2b-8e4b-11781025b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_model_related_metrics(original_p, counter_p):\n",
    "#     \"\"\"\n",
    "#     A function that takes as input the original predictions and the new ones, and returns a dictionary with \n",
    "#     model-related metrics such as flip-rate.\n",
    "\n",
    "#     :param original_p: list containing the predictions for the original data\n",
    "#     :param counter_p: dataframe containing the predictions for the counter data\n",
    "#     :returns: dictionary containing model-related metrics\n",
    "#     \"\"\"\n",
    "\n",
    "#     # check that predictions and counter_predictions are of the same length\n",
    "#     assert len(original_p) == len(counter_p)\n",
    "    \n",
    "#     # compute flip_rate\n",
    "#     flip_rate_percent = sum(x[0] != x[1] for x in zip(original_p, counter_p)) / len(original_p)\n",
    "\n",
    "#     # create metrics dictionary\n",
    "#     metrics = {\n",
    "#         'flip-rate': flip_rate_percent\n",
    "#     }\n",
    "\n",
    "#     return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5c6b7e0-426c-470e-a7ed-9b1d04853969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counterfactual_metric(metrics):\n",
    "    \"\"\"\n",
    "    A function that takes as input a dictionary containing evalutation metrics, and returns\n",
    "    a combination of those metrics.\n",
    "\n",
    "    :param metrics: dictionary containing different evaluation metrics such as fluency, flip-rate, etc.\n",
    "    :returns: float value, computed as a combination of the metrics in the given dictionary\n",
    "    \"\"\"\n",
    "    return metrics['fluency'] if metrics['fluency'] is not None else 0\n",
    "    #return 2 / (1/metrics['bertscore'] + 1/metrics['fluency'])\n",
    "    #return len(metrics) / sum(1/v for v in metrics.values())   # compute final metric as the harmonic mean of the given metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05116c5e-6823-452c-9bf4-c94612059d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_metric(data, pos, eval_metric, model_required=False, preprocessor=None, model=None, antonyms=False):\n",
    "    \"\"\"\n",
    "    A function that takes as input a dataframe with the textual data, and computes a metric based on a bipartite graph,\n",
    "    where the edge weights represent the distance between words (nodes) as extracted from wordnet.\n",
    "\n",
    "    :param data: pd.DataFrame() containing one column with the textual data\n",
    "    :param pos: string that specifies which part-of-speech shall be considered for substitution (noun, verb, adv)\n",
    "    :param eval_metric: a function that computes the metric which must be optimized during fine-tuning\n",
    "    :param model_required: boolean value specifing whether a pretrained model is also required for the metric computation\n",
    "    :param preprocessor: a custom class that implements the necessary preprocessing of the data\n",
    "    :param model: a pretrained model on the dataset \n",
    "    :returns: a float value representing the computed evaluation metric\n",
    "    \"\"\"\n",
    "    \n",
    "    sents = [elem[0] for elem in data.values.tolist()]\n",
    "    counter_sents, _, _, _= get_edits(sents, pos=pos, thresh=3, antonyms=antonyms)\n",
    "    \n",
    "    counter_data_df = pd.DataFrame({\n",
    "        'counter_sents': counter_sents\n",
    "    })\n",
    "\n",
    "    # print('Generating Model Agnostic Metrics...')\n",
    "    # metrics = generate_model_agnostic_metrics(data, counter_data_df)\n",
    "    if model_required == False:\n",
    "        return eval_metric(data, counter_data_df), counter_data_df\n",
    "\n",
    "    else:\n",
    "        # first process the original data and get model predictions\n",
    "        processed_data = preprocessor.process(data)\n",
    "        original_preds = model.predict(processed_data)\n",
    "    \n",
    "        # do the same but for the counterfactual-generated data\n",
    "        processed_counter_data = preprocessor.process(counter_data_df)\n",
    "        counter_preds = model.predict(processed_counter_data)\n",
    "    \n",
    "        return eval_metric(original_preds, counter_preds), counter_data_df\n",
    "        \n",
    "    # return get_counterfactual_metric(metrics), counter_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5b9a9-cb3d-4343-add6-a26ac06be324",
   "metadata": {},
   "source": [
    "## Graph-Related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38744b17-f2fd-4244-b94d-7bcb333444e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(data, pos, antonyms=False):\n",
    "    \"\"\"\n",
    "    A function that takes as input a dataframe and a part-of-speech tag, and creates a bipartite graph\n",
    "    with the possible substitution words and their candidates.\n",
    "\n",
    "    :param data: pd.DataFrame() containing one column with the textual data\n",
    "    :param pos: string that specifies which part-of-speech shall be considered for substitution (noun, verb, adv)\n",
    "    :param antonyms: boolean value specifing whether or not to use antonyms in the candidate substitutions \n",
    "    :returns: a dictionary containing the graph, along with other related features\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = [elem[0] for elem in data.values.tolist()]\n",
    "    lst = None\n",
    "    \n",
    "    # use appropriate function based on pos to get the list of the specified pos words from the data\n",
    "    if pos == 'adv':\n",
    "        lst = create_attributes_list(sentences)\n",
    "    elif pos == 'verb':\n",
    "        lst = create_verb_list(sentences)\n",
    "    elif pos == 'noun':\n",
    "        lst = create_singular_list(sentences) \n",
    "    else:\n",
    "        raise AttributeError(\"pos '{}' is not supported!\".format(pos)) \n",
    "    \n",
    "    weights = []\n",
    "    syn0 = list(lst)\n",
    "    syn1 = list(get_antonym_list(lst)) if antonyms else list(lst)\n",
    "        \n",
    "    all_syn0, d0, ind0 = get_synsets(syn0, pos=pos, return_index=True)\n",
    "    all_syn1, d1, ind1= get_synsets(syn1, pos=pos return_index=True)\n",
    "    \n",
    "    print(\"Creating Node Names...\")\n",
    "    names0 = ['G0_'+str(i) for i in range(len(all_syn0))]  # give unique names for each synset of the two sets\n",
    "    names1 = ['G1_'+str(i) for i in range(len(all_syn1))]\n",
    "\n",
    "    word_to_node0 = dict()\n",
    "    word_to_node1 = dict()\n",
    "    for t in zip(names0, ind0):\n",
    "        word_to_node0[syn0[t[1]]] = t[0]\n",
    "\n",
    "    for t in zip(names1, ind1):\n",
    "        word_to_node1[syn1[t[1]]] = t[0]\n",
    "        \n",
    "    \n",
    "    # synset as key, word as val\n",
    "    combinations_nodes = all_combinations(names0, names1)        # all combinations of names\n",
    "    combinations_synsets = all_combinations(all_syn0, all_syn1)  # all combinations of synsets\n",
    "    weights = [1] * len(combinations_nodes)\n",
    "\n",
    "    print(\"Creating Bipartite Graph...\")\n",
    "    G, min_list_nodes = bipartite_graph(names0, names1, combinations_nodes, weights) # create bipartite graph\n",
    "\n",
    "    graph_dict = {\n",
    "        'graph': G,\n",
    "        'min_list_nodes': min_list_nodes,\n",
    "        'weights': weights,\n",
    "        'd0': d0,\n",
    "        'd1': d1,\n",
    "        'comb_nodes': combinations_nodes,\n",
    "        'comb_syn': combinations_synsets,\n",
    "        'word_to_node0': word_to_node0,\n",
    "        'word_to_node1': word_to_node1\n",
    "    }\n",
    "\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0585d7ff-4bc7-40e9-80c3-c15b3a282cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_matching(graph_dict):\n",
    "    \"\"\"\n",
    "    A function that takes as input a dictionary containing a graph and other related features, and uses\n",
    "    a minimum graph matching algorithm to return candidate substitutions, along with other graph features.\n",
    "\n",
    "    :param graph_dict: a dictionary containing a bipartite graph and other related features\n",
    "    :returns: a list of feasible substitutions, mappings of synsets to their words, and a tuple containing the graph, a min_list_nodes and the minimum matching\n",
    "    \"\"\"\n",
    "    \n",
    "    # unpack dictionary items\n",
    "    G = graph_dict['graph']\n",
    "    min_list_nodes = graph_dict['min_list_nodes']\n",
    "    weights = graph_dict['weights']\n",
    "    d0 = graph_dict['d0']\n",
    "    d1 = graph_dict['d1']\n",
    "    combinations_nodes = graph_dict['comb_nodes']\n",
    "    combinations_synsets = graph_dict['comb_syn']\n",
    "\n",
    "    # find min weight match\n",
    "    print(\"Finding Minimum Match...\")\n",
    "    min_match = minimum_match(G, min_list_nodes)                                     \n",
    "    match_tuple = dict_to_tuple(min_match)\n",
    "    \n",
    "    new_match=[]\n",
    "    for i in match_tuple:\n",
    "        new_match.append(tuple(sorted(i)))\n",
    "        new_match = remove_duplicates(new_match)\n",
    "\n",
    "    positions = pos_in_list(combinations_nodes, list(new_match))\n",
    "    # substitution_synsets = []\n",
    "    substitution_synsets = dict()\n",
    "    print(\"Creating Substitution Synsets Dictionary...\")\n",
    "    for i in positions:\n",
    "        # substitution_synsets.append((weights[i], combinations_synsets[i][0], combinations_synsets[i][1])) \n",
    "        substitution_synsets[d0[combinations_synsets[i][0]]] = d1[combinations_synsets[i][1]]\n",
    "        substitution_synsets[d1[combinations_synsets[i][1]]] = d0[combinations_synsets[i][0]]\n",
    "    # sum_similarities, avg_similarity, best_matched_synsets = total_graph_weight(positions, weights, combinations_synsets)\n",
    "        \n",
    "    return substitution_synsets, d0, d1, (G, min_list_nodes, new_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1558be4-13c0-4b7d-b9df-1d738af179af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_counterfactuals(graph_dict, data, pos):\n",
    "    \"\"\"\n",
    "    A function that takes as input a dictionary containing graph information, along with a dataframe and a part-of-speech tag,\n",
    "    and uses them to generate counterfactual edits from the data.\n",
    "\n",
    "    :param graph_dict: a dictionary containing a bipartite graph and other related features\n",
    "    :param data: pd.DataFrame() containing one column with the textual data\n",
    "    :param pos: string that specifies which part-of-speech shall be considered for substitution (noun, verb, adv)\n",
    "    :returns: a dataframe with the generated counterfactual data, a list of selected edges from the graph and a dictionary containing substitution occurrence\n",
    "    \"\"\"\n",
    "    \n",
    "    G = graph_dict['graph']\n",
    "    w2n0 = graph_dict['word_to_node0']\n",
    "    w2n1 = graph_dict['word_to_node1']\n",
    "    sentences = [elem[0] for elem in data.values.tolist()] \n",
    "    \n",
    "    # find best matching and generate edits\n",
    "    substitution_synsets, d0, d1, g = generate_graph_matching(graph_dict)\n",
    "    print(\"Generating Edits...\")\n",
    "    all_swaps, if_change, attr_counter, substitutions = external_swaps(sentences, pos, substitution_synsets, d0, d1, thresh=3)\n",
    "    \n",
    "\n",
    "    counter_data = pd.DataFrame({\n",
    "        'counter_sents': all_swaps\n",
    "    })\n",
    "\n",
    "    subs_as_nodes = dict()\n",
    "    for (k,v) in substitutions.items():\n",
    "        try:\n",
    "            subs_as_nodes[(w2n0[k[0]], w2n1[k[1]])] =  v\n",
    "        except KeyError:\n",
    "            subs_as_nodes[(w2n0[k[1]], w2n1[k[0]])] =  v\n",
    "\n",
    "    selected_edges = []\n",
    "    for (u,v) in subs_as_nodes.keys():\n",
    "        w = G.get_edge_data(u, v, default=0)['weight']\n",
    "        selected_edges.append((u, v, w))\n",
    "\n",
    "    return counter_data, selected_edges, subs_as_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "723864c4-0a26-4c30-875c-af9cf90130e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(graph_dict, data, pos, eval_metric, preprocessor=None, model=None, learning_rate=0.1, th=0.005, max_iterations=100, model_required=False, baseline_metric=None):\n",
    "    \"\"\"\n",
    "    A function that represents the training process for the graph edges. It gets predictions for the original data\n",
    "    then uses a graph approach to generate counter data and get predictions for them. To get the current_metric\n",
    "    it compares the two predictions and based on those updates the weights of the selected edges.\n",
    "    \n",
    "    :param graph_dict: a dictionary containing the bipartite graph along with other variables and characteristics\n",
    "    :param data: a dataframe containing the textual examples we will use to train the graph\n",
    "    :param pos: a string specifing which part-of-speech shall be considered for substitutions (noun, verb, adv)\n",
    "    :param eval_metric: a function that computes the metric which must be optimized during fine-tuning\n",
    "    :param preprocessor: a custom class that implements the necessary preprocessing of the data\n",
    "    :param model: a pretrained model on the dataset\n",
    "    :param learning_rate: float value defining how fast or slow the edge weights will be updated\n",
    "    :param th: float value defining a threshold, where if the difference |baseline - current| get smaller, the training stops\n",
    "    :param max_iterations: integer value representing the maximum number of iterations for the training procedure\n",
    "    :param model_required: boolean value for whether or not to compute model-related metrics\n",
    "    :returns: the graph_dictionary with the fine-tuned (post-training) graph along with the rest of its features\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize baseline and current metric\n",
    "    if baseline_metric is None:\n",
    "        baseline_metric = get_baseline_metric(data, pos=pos, eval_metric=eval_metric, model_required=model_required, preprocessor=preprocessor, model=model)[0] \n",
    "    current_metric = baseline_metric + 2 * th   # initialize current_metric so that the dif |baseline-current| is bigger than th\n",
    "    \n",
    "    iterations = 0\n",
    "    next_baseline_metric = baseline_metric\n",
    "    while abs(current_metric - baseline_metric) >= th and iterations < max_iterations:\n",
    "        print(\"ITERATION {}\".format(iterations))\n",
    "\n",
    "        updated_edges = []\n",
    "        baseline_metric = next_baseline_metric\n",
    "\n",
    "        while nx.is_bipartite(graph_dict['graph']):\n",
    "            try:\n",
    "                counter_data, selected_edges, substitutions = generate_counterfactuals(graph_dict, data, pos)\n",
    "                \n",
    "                if model_required == False:\n",
    "                # compute current_metric valule\n",
    "                    current_metric = eval_metric(data, counter_data)\n",
    "                    \n",
    "                else:\n",
    "                    processed_counter_data = preprocessor.process(counter_data)\n",
    "                    counter_preds = model.predict(processed_counter_data)\n",
    "            \n",
    "                    # compute model-related current_metric value\n",
    "                    current_metric = eval_metric(original_preds, counter_preds) \n",
    "        \n",
    "                # compute the final metric as a combination of the previously computed metrics\n",
    "                # current_metric = get_counterfactual_metric(current_metrics_dict)\n",
    "    \n",
    "                g = graph_dict['graph']\n",
    "                g.remove_edges_from(selected_edges)\n",
    "                new_edges = update_edges(selected_edges, substitutions, learning_rate, baseline_metric, current_metric)\n",
    "                \n",
    "                graph_dict['graph'] = g\n",
    "                updated_edges.extend(new_edges)\n",
    "            except:\n",
    "                graph_dict['graph'] = g\n",
    "                break\n",
    "            \n",
    "        g = graph_dict['graph']\n",
    "        # print(updated_edges)\n",
    "        g.add_weighted_edges_from(updated_edges)\n",
    "        graph_dict['graph'] = g\n",
    "\n",
    "        # update baseline_metric value and iterations\n",
    "        next_baseline_metric = min(baseline_metric, current_metric)\n",
    "        iterations += 1\n",
    "\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa73f1-ab9f-416c-9d71-5580febe9f3a",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db9e3960-f3b9-41c8-9ac2-e6c88a209a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS = 'adv'\n",
    "# MAX_ITER = 3\n",
    "# ANTONYMS = True\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'sents': [\n",
    "#         'A great man was standing in a tall and magnificent hill, gazing upon the sad and destructive army',\n",
    "#         'The clever boy was wondering when the fat dog would return with the big stick',\n",
    "#         'A small town was standing next to the large river and the tall building'\n",
    "#     ]\n",
    "# })\n",
    "\n",
    "# gd = create_graph(data=df, pos=POS, antonyms=ANTONYMS)\n",
    "# trained_gd = train_graph(graph_dict=gd, data=df, pos=POS, max_iterations=MAX_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a7c13fa-c458-4c7a-9756-d5e619c438c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = \"\"\"\n",
    "# I went and saw this movie last night after being coaxed to by a few friends of mine. \n",
    "# I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy.\n",
    "# I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. \n",
    "# The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was \n",
    "# overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I \n",
    "# not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great,\n",
    "# and I suggest that you go see it before you judge.\"\"\"\n",
    "\n",
    "# x2 = \"\"\"\n",
    "# Maybe I'm reading into this too much, but I wonder how much of a hand Hongsheng had in developing the film. I mean, when a story is told casting the\n",
    "# main character as himself, I would think he would be a heavy hand in writing, documenting, etc. and that would make it a little biased.<br /><br />\n",
    "# But...his family and friends also may have had a hand in getting the actual details about Hongsheng's life. I think the best view would have been told\n",
    "# from Hongsheng's family and friends' perspectives. They saw his transformation and weren't so messed up on drugs that they remember everything.<br /><br />As\n",
    "# for Hongsheng being full of himself, the consistencies of the Jesus Christ pose make him appear as a martyr who sacrificed his life (metaphorically, of course, \n",
    "# he's obviously still alive as he was cast as himself) for his family's happiness. Huh?<br /><br />The viewer sees him at his lowest points while still maintaining \n",
    "# a superiority complex. He lies on the grass coming down from (during?) a high by himself and with his father, he contemplates life and has visions of dragons at his\n",
    "# window, he celebrates his freedom on a bicycle all while outstretching his arms, his head cocked to the side.<br /><br />It's fabulous that he's off of drugs now, but \n",
    "# he's no hero. He went from a high point in his career in acting to his most vulnerable point while on drugs to come back somewhere in the middle.<br /><br />This same \n",
    "# device is used in Ted Demme's \"Blow\" where the audience empathizes with the main character who is shown as a flawed hero.<br /><br />However, \"Quitting\" (\"Zuotian\") is a \n",
    "# film that is recommended, mostly for its haunting soundtrack, superb acting, and landscapes. But, the best part is the feeling that one gets when what we presume to be the\n",
    "# house of Jia Hongsheng is actually a stage setting for a play. It makes the viewer feel as if Hongsheng's life was merely a play told in many difficult parts.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fed4d233-70f9-45af-a055-e1c6380f494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# start = datetime.now()\n",
    "# ld = lev_dist(x1, x2)\n",
    "# print(\"Command Execution Time: {}\".format(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f1f84e-2a53-42e0-92db-ec72cf720556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter_data, selected_edges, subs = generate_counterfactuals(trained_gd, df, POS)\n",
    "# for i in range(df.shape[0]):\n",
    "#     print(\"ORIGINAL:\")\n",
    "#     print(df['sents'][i])\n",
    "#     print(\"COUNTER:\")\n",
    "#     print(counter_data['counter_sents'][i])\n",
    "#     print(\"===============================================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "291ba437-c82a-4b60-a200-a61e21b4efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_metric = get_counterfactual_metric(generate_model_agnostic_metrics(df, counter_data))\n",
    "# baseline_metric = get_baseline_metric(df, pos=POS)\n",
    "\n",
    "# print(\"Baseline metric value: {}\".format(baseline_metric))\n",
    "# print(\"Fine-tuned metric value: {}\".format(final_metric))\n",
    "# print(\"Difference: {}\".format(abs(baseline_metric - final_metric)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
