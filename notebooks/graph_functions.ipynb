{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38161142-4702-4721-a5cd-8ad856cb63a4",
   "metadata": {},
   "source": [
    "# Graph Framework Functions\n",
    "<b>Date:</b> October 20, 2023 \\\n",
    "<b>Author:</b> Dimitris Lymperopoulos \\\n",
    "<b>Description:</b> A notebook containing functions related with the bipartite graph framework for edits generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f816720-317d-4c63-91de-ec7693b4e55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import itertools\n",
    "import collections\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.parse import (DependencyGraph,ProjectiveDependencyParser,NonprojectiveDependencyParser)\n",
    "\n",
    "from itertools import permutations\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1f1b7-8672-48d0-9ae6-821019cce4e5",
   "metadata": {},
   "source": [
    "## General Helping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842a3844-5ace-4b19-a19a-9d324660ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_noun(sentence):\n",
    "    singular = []\n",
    "    plural = []\n",
    "    # expressions containing nouns that should not be substituted\n",
    "    exceptions = ['a group of', 'a couple of', 'group of', 'couple of', 'many', 'several', 'a lot of', 'lots of', 'others', 'other']\n",
    "    expressions_to_pass = ['in front of', 'next to']   \n",
    "    txt = sentence.lower()\n",
    "    for e in (exceptions+expressions_to_pass):\n",
    "        txt=txt.replace(e,'')\n",
    "\n",
    "    txt = ' '.join(txt.split())      # remove multiple whitespaces\n",
    "    doc = nlp(txt)\n",
    "    for token in doc:\n",
    "        if len(token)>1:\n",
    "            if token.tag_=='NN':\n",
    "                singular.append(token.text)\n",
    "            elif token.tag_=='NNS':\n",
    "                plural.append(token.text)\n",
    "    return singular, plural, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f842ec-c2bf-459e-a02b-9684d4b91c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_verb(txt):\n",
    "    vbp = []\n",
    "    vbg = []\n",
    "    vb = []\n",
    "    txt = txt.lower()\n",
    "    #for e in exceptions:\n",
    "    #  txt=txt.replace(e,'')\n",
    "    txt=txt.strip()\n",
    "    doc = nlp(txt)\n",
    "    for token in doc:\n",
    "        if token.dep_!='aux':         # except auxiliary verbs\n",
    "          #print(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_)\n",
    "            if token.tag_=='VBP':\n",
    "                #print(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_)\n",
    "                vbp.append(token.text)\n",
    "            elif token.tag_=='VBG':\n",
    "                #print(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_)\n",
    "                vbg.append(token.text)\n",
    "            elif token.tag_=='VB':\n",
    "                #print(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_)\n",
    "                vb.append(token.text)\n",
    "\n",
    "    return vbp, vbg, vb, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c557511-ba1a-44f7-8d1d-65a70066124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_attribute(sentence):\n",
    "    # expressions containing nouns that should not be substituted\n",
    "    txt = sentence.lower()\n",
    "    txt = ' '.join(txt.split())      # remove multiple whitespaces\n",
    "    doc = nlp(txt)\n",
    "    attr_list = []\n",
    "    for token in doc:\n",
    "        if token.pos_=='ADJ':\n",
    "            attr_list.append(token.text)\n",
    "        #print(token.text, token.tag_, token.pos)\n",
    "    return attr_list, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82fb7eef-e825-4d71-84c6-7fe0f5a31abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_words(s, x, y):\n",
    "    return y.join(part.replace(y, x) for part in s.split(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012f77c3-d786-424c-8395-c127a4349b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_verbs(caption, l, change, nouns_changed):\n",
    "    if len(l)>1:\n",
    "        new_sent = swap_words(caption, l[0], l[1])\n",
    "        nouns_changed.append(l[0])\n",
    "        nouns_changed.append(l[1])\n",
    "        change+=1\n",
    "        \n",
    "    else:\n",
    "        new_sent=caption\n",
    "\n",
    "    return new_sent, change, nouns_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e1f4f65-13ce-4d4d-9132-7d8727a6473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_changed(val, lst):\n",
    "    if val>0:\n",
    "        lst.append(1)\n",
    "        \n",
    "    else:\n",
    "        lst.append(0)\n",
    "        \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85424b68-7588-4e96-afd5-7e4dcbb497fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ends_with_fullstop(txt):\n",
    "    if txt.strip().endswith('.'):\n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        txt+='.'\n",
    "        \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6da7025-8487-4c90-9271-60780e0ad878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_diff(l1, l2):\n",
    "    diff = [x for x in l1 if x not in set(l2)]\n",
    "    \n",
    "    return diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0b321-a5f8-4212-9c22-2362f5744588",
   "metadata": {},
   "source": [
    "## More Specific helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f05c9c21-6e52-4690-9ee4-15c3deb3d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synsets(syn, return_index=False):    \n",
    "    all_syn=[]\n",
    "    indices = []\n",
    "    d= dict()\n",
    "    for idx, i in enumerate(syn):\n",
    "        if wn.synsets(i, pos='n')!=[]:\n",
    "            s = wn.synsets(i, pos='n')[0]\n",
    "            all_syn.append(s)\n",
    "            d[s] = i\n",
    "            indices.append(idx)\n",
    "    if return_index:\n",
    "        return all_syn, d, indices\n",
    "    else:\n",
    "        return all_syn, d\n",
    "\n",
    "\n",
    "def get_antonym(given_word):\n",
    "    antonyms = []\n",
    "    for syn in wn.synsets(given_word):\n",
    "        for l in syn.lemmas():\n",
    "            if l.antonyms() and l.antonyms()[0].synset().pos() == l.synset().pos():\n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "    return list(set(antonyms))   # remove duplicates and return the list\n",
    "\n",
    "\n",
    "def get_antonym_list(words):\n",
    "    ant_list = []\n",
    "    for w in words:\n",
    "        ant_list.extend(get_antonym(w))\n",
    "    return ant_list\n",
    "\n",
    "def all_combinations(a, b):   # give all combinations of two sets\n",
    "    combinations = list(itertools.product(a, b))   # cartesian product\n",
    "    return combinations\n",
    "\n",
    "\n",
    "def shorter_list(l1, l2):\n",
    "    min_list = min([l1, l2], key=len)  # find the shortest list between two lists\n",
    "    return min_list\n",
    "\n",
    "\n",
    "def swap(item):\n",
    "    swaped = item[1], item[0]\n",
    "    return swaped\n",
    "\n",
    "\n",
    "def dict_to_tuple(d):\n",
    "    tmp = d.items()   # get dict items\n",
    "    l = list(tmp)     # convert dict to list of tuples (k, v)\n",
    "    return l\n",
    "\n",
    "\n",
    "def remove_duplicates(l):\n",
    "    l = list(set(l))\n",
    "    return l\n",
    "\n",
    "\n",
    "def pos_in_list(l, m):\n",
    "    positions = []\n",
    "    for i in m:\n",
    "        swaped = swap(i)\n",
    "        if i in l:\n",
    "            positions.append(l.index(i))\n",
    "        elif swaped in l:                   # as an undirected graph, swaped edges are the same as in normal order\n",
    "            positions.append(l.index(swaped))\n",
    "        else:\n",
    "            print(i, 'not in list')\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d16bb-da00-4afa-af29-8882672c16d3",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42ee9974-4a6b-43fd-bf21-14e99ede5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_graph_weight(positions, weights, combinations_synsets):  # cumulative weight of bipartite matches\n",
    "    sum_similarities = 0\n",
    "    best_matched_synsets = []\n",
    "    \n",
    "    for i in positions:\n",
    "        w = weights[i]\n",
    "        sum_similarities+=w\n",
    "        synset_pair = combinations_synsets[i]\n",
    "        best_matched_synsets.append([synset_pair[0], synset_pair[1],w])\n",
    "        \n",
    "    avg_similarity = sum_similarities/len(positions)\n",
    "    \n",
    "    return sum_similarities, avg_similarity, best_matched_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03fc17c4-73e9-49ca-a4be-279911d6d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipartite_graph(names0, names1, combinations_n, weights):\n",
    "    G = nx.Graph()\n",
    "    G0_nodes = names0\n",
    "    G1_nodes = names1\n",
    "    min_list_nodes = shorter_list(names0, names1)\n",
    "    \n",
    "    G.add_nodes_from(G0_nodes, bipartite=0)\n",
    "    G.add_nodes_from(G1_nodes, bipartite=1)\n",
    "    \n",
    "    for name, w in zip(combinations_n, weights):\n",
    "        G.add_edge(name[0], name[1], weight = w)\n",
    "        \n",
    "    if not nx.is_bipartite(G):\n",
    "        print('Graph is not bipartite')\n",
    "        \n",
    "    return G, min_list_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69108e45-7539-417a-acd3-5b203ae10090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_match(G, min_list_nodes):\n",
    "    min_matching = bipartite.matching.minimum_weight_full_matching(G, min_list_nodes, \"weight\")\n",
    "    return min_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01ef7fdc-50e3-4acb-9433-87a26b2db26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_path_similarity(synset0, synset1):    # find wordnet path similarity score between two given synsets\n",
    "    sim = synset0.path_similarity(synset1)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3db785ad-5a43-47f5-aeff-e533e79b2b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_hierarchy(s0, s1, baseline=True):\n",
    "    weights = []\n",
    "    syn0 = list(s0)\n",
    "    syn1 = list(s1)\n",
    "    all_syn0, d0 = get_synsets(syn0)\n",
    "    all_syn1, d1 = get_synsets(syn1)\n",
    "    names0 = ['G0_'+str(i) for i in range(len(all_syn0))]  # give unique names for each synset of the two sets\n",
    "    names1 = ['G1_'+str(i) for i in range(len(all_syn1))]\n",
    "    \n",
    "    # synset as key, word as val\n",
    "    combinations_nodes = all_combinations(names0, names1)        # all combinations of names\n",
    "    combinations_synsets = all_combinations(all_syn0, all_syn1)  # all combinations of synsets\n",
    "    if baseline:\n",
    "        for comb, syn in zip(combinations_nodes, combinations_synsets): # find path similarities for all combinations\n",
    "            path_sim = wn_path_similarity(syn[0], syn[1])\n",
    "            weights.append(path_sim)                                    # with those pairwise similarities acting as weights\n",
    "    else:\n",
    "        weights = [1] * len(combinations_nodes)\n",
    "   \n",
    "    G, min_list_nodes = bipartite_graph(names0, names1, combinations_nodes, weights) # create bipartite graph\n",
    "    min_match = minimum_match(G, min_list_nodes)                                     # find min weight match\n",
    "    match_tuple = dict_to_tuple(min_match)\n",
    "    \n",
    "    new_match=[]\n",
    "    for i in match_tuple:\n",
    "        # new_match.append(tuple(sorted(i)))\n",
    "        # new_match = remove_duplicates(new_match)\n",
    "        new_match.append(i)\n",
    "\n",
    "    positions = pos_in_list(combinations_nodes, list(new_match))\n",
    "    substitution_synsets = []\n",
    "    \n",
    "    for i in positions:\n",
    "        substitution_synsets.append((weights[i], combinations_synsets[i][0], combinations_synsets[i][1]))\n",
    "        \n",
    "    sum_similarities, avg_similarity, best_matched_synsets = total_graph_weight(positions, weights, combinations_synsets)\n",
    "    \n",
    "    return substitution_synsets, d0, d1, (G, min_list_nodes, new_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13a1f1c2-0df9-480f-9e66-fce803ba2771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_select_triplet(lst):\n",
    "    s_list = sorted(lst, key=itemgetter(0))   # sort list based on 1st element (weight). Smaller values come first\n",
    "    selected = s_list[0]                      # get lower similarity triplet (first after sorting)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cedef1ba-3a20-4e3b-adfb-6a16a1db26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_dissimilar_pair(substitution_synsets, d0, d1):\n",
    "    selected = sort_select_triplet(substitution_synsets)  # selected format: (weight, synset0, synset1)\n",
    "    p0 = d0[selected[1]]                                  # 1st synset at position 1\n",
    "    p1 = d1[selected[2]]                                  # 2nd synset at position 2\n",
    "    subs_pair = (p0, p1)\n",
    "    return subs_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7341049b-646f-41d5-940c-6714e1707553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attributes_list(sentences):\n",
    "    all_attributes=[]\n",
    "    for s in sentences:\n",
    "            attribute, new_s = check_if_attribute(s)\n",
    "            all_attributes.append(attribute)\n",
    "            \n",
    "    attributes = [item for sublist in all_attributes for item in sublist]\n",
    "    attributes = list(set(attributes))\n",
    "    attributes = [word.replace('\\\\n', '') for word in attributes]\n",
    "    attributes = [word.replace('\\\\', '') for word in attributes]\n",
    "\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fd61e14-7f9d-4ce6-bd60-2c19b3ec23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_adverb_substitutions(sentences, baseline=True, antonyms=False):\n",
    "    \"\"\"\n",
    "    A function that takes as input a list od sentences, and generates substitutions.\n",
    "\n",
    "    :param sentences: Iterable containing the sentences that will be changed\n",
    "    :param antonyms: boolean value specifing whether or not to use antonyms in the candidate substitutions\n",
    "    :returns: the substitutions, the source set and the target set of the bipartite graph, the graph and the minimum_node_list\n",
    "    \"\"\"\n",
    "    \n",
    "    attributes = create_attributes_list(sentences)\n",
    "    if antonyms:\n",
    "        return wn_hierarchy(attributes, get_antonym_list(attributes), baseline)\n",
    "    else:\n",
    "        return wn_hierarchy(attributes, attributes, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa38b166-958d-4c3d-99bc-b7cce337712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_verb_list(sentences):\n",
    "    all_vbp=[]\n",
    "    all_vbg = []\n",
    "    all_vb = []\n",
    "    \n",
    "    for s in sentences:\n",
    "        vbp, vbg, vb, new_s = check_if_verb(s)\n",
    "        all_vbp.append(vbp)\n",
    "        all_vbg.append(vbg)\n",
    "        all_vb.append(vb)\n",
    "            \n",
    "    vbp = [item for sublist in all_vbp for item in sublist]\n",
    "    vbp = list(set(vbp))\n",
    "    vbp = [word.replace('\\\\n', '') for word in vbp]\n",
    "    vbp = [word.replace('\\\\', '') for word in vbp]\n",
    "    \n",
    "    vbg = [item for sublist in all_vbg for item in sublist]\n",
    "    vbg = list(set(vbg))\n",
    "    vbg = [word.replace('\\\\n', '') for word in vbg]\n",
    "    vbg = [word.replace('\\\\', '') for word in vbg]\n",
    "    \n",
    "    vb = [item for sublist in all_vb for item in sublist]\n",
    "    vb = list(set(vb))\n",
    "    vb = [word.replace('\\\\n', '') for word in vb]\n",
    "    vb = [word.replace('\\\\', '') for word in vb]\n",
    "\n",
    "    return vbp+vbg+vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ced1afb5-41f2-4a24-a68f-2d116e347297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_verb_substitutions(sentences, baseline=True, antonyms=False):\n",
    "    \"\"\"\n",
    "    A function that takes as input a list od sentences, and generates substitutions.\n",
    "\n",
    "    :param sentences: Iterable containing the sentences that will be changed\n",
    "    :param antonyms: boolean value specifing whether or not to use antonyms in the candidate substitutions\n",
    "    :returns: the substitutions, the source set and the target set of the bipartite graph, the graph and the minimum_node_list\n",
    "    \"\"\"\n",
    "    \n",
    "    verb_list = create_verb_list(sentences)\n",
    "    if antonyms:\n",
    "        return wn_hierarchy(verb_list, get_antonym_list(verb_list), baseline)\n",
    "    else:\n",
    "        return wn_hierarchy(verb_list, verb_list, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11a771b7-001a-4ace-a7d4-d22ecdc0d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_singular_list(sentences):\n",
    "    all_singulars=[]\n",
    "    #all_plurals = []\n",
    "    \n",
    "    for s in sentences:\n",
    "        singular, plural, new_s = check_if_noun(s)\n",
    "        all_singulars.append(singular)\n",
    "        #all_plurals.append(plural)\n",
    "            \n",
    "    singulars = [item for sublist in all_singulars for item in sublist]\n",
    "    singulars = list(set(singulars))\n",
    "    singulars = [word.replace('\\\\n', '') for word in singulars]\n",
    "    singulars = [word.replace('\\\\', '') for word in singulars]\n",
    "    \n",
    "    # all_plurals = [item for sublist in all_plurals for item in sublist]\n",
    "    # plurals = list(set(all_plurals))\n",
    "    # plurals = [word.replace('\\\\n', '') for word in plurals]\n",
    "    # plurals = [word.replace('\\\\', '') for word in plurals]\n",
    "\n",
    "    return singulars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c3ba5c6-960e-4dd7-8996-0b4aa3e16683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_noun_substitutions(sentences, baseline=True, antonyms=False):\n",
    "    \"\"\"\n",
    "    A function that takes as input a list od sentences, and generates substitutions.\n",
    "\n",
    "    :param sentences: Iterable containing the sentences that will be changed\n",
    "    :param antonyms: boolean value specifing whether or not to use antonyms in the candidate substitutions\n",
    "    :returns: the substitutions, the source set and the target set of the bipartite graph, the graph and the minimum_node_list\n",
    "    \"\"\"\n",
    "    \n",
    "    singulars = create_singular_list(sentences)\n",
    "    if antonyms:\n",
    "        return wn_hierarchy(singulars, get_antonym_list(singulars), baseline)\n",
    "    else:\n",
    "        return wn_hierarchy(singulars, singulars, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1081b969-4fd2-417e-8b57-7df9f7da7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_swaps(sentences, pos, substitution_singular, d0_s, d1_s, thresh=100):\n",
    "    \"\"\"\n",
    "    A function that takes as input a dataframe and the name of the column where the sentences are,\n",
    "    and generates substitutions.\n",
    "\n",
    "    :param sentences: Iterable containing the sentences that will be changed\n",
    "    :param pos: a string specifing which part-of-speech shall be changed (attr, verb or noun)\n",
    "    :param substitution_singular: an iterable with the possible substitutions\n",
    "    :param d0_s: a list containing the nodes (words) of the source set of the bipartite graph\n",
    "    :param d1_s: a list containing the nodes (words) of the target set of the bipartite graph\n",
    "    :param thresh: Integer representing how many substitutions in each sentence shall occurr\n",
    "    :returns: a triplet containing the swaps that were made, a list denoting which sentences were changed and how many attributes were changed\n",
    "    \"\"\"\n",
    "    all_swaps = []\n",
    "    if_change = []\n",
    "    substitutions = dict()\n",
    "    attr_counter = 0\n",
    "    \n",
    "    for s in sentences:\n",
    "        change = 0\n",
    "        txt = s.lower().replace('\\\\n', '')\n",
    "\n",
    "        # according to the pos given, use the appropriate function to create the list with candidate words to be substituted\n",
    "        if pos == 'adv':\n",
    "            candidate_list, new_s = check_if_attribute(s)\n",
    "        elif pos == 'verb':\n",
    "            vbp, vbg, vb, new_c = check_if_verb(txt)\n",
    "            candidate_list = vbp + vbg + vb\n",
    "        elif pos == 'noun':\n",
    "            candidate_list, plural, new_c = check_if_noun(txt)\n",
    "        else:\n",
    "            raise AttributeError(\"pos '{}' is not supported!\".format(pos))\n",
    "            \n",
    "        # crop candidate list if it is larger than the threshold given\n",
    "        if len(candidate_list)>thresh:\n",
    "            candidate_list = candidate_list[:thresh]\n",
    "\n",
    "        # perform substitutions \n",
    "        for i in substitution_singular:\n",
    "            if d0_s[i[1]] in txt and (d0_s[i[1]] in candidate_list):\n",
    "                txt=re.sub(r\"\\b%s\\b\" % d0_s[i[1]] , d1_s[i[2]], txt)\n",
    "                change+=1\n",
    "                attr_counter+=1\n",
    "                substitutions[(d0_s[i[1]], d1_s[i[2]])] = substitutions.get((d0_s[i[1]], d1_s[i[2]]), 0) + 1\n",
    "                \n",
    "            elif d1_s[i[2]] in txt and (d1_s[i[2]] in candidate_list):\n",
    "                txt = txt.replace(d1_s[i[2]], d0_s[i[1]])\n",
    "                txt=re.sub(r\"\\b%s\\b\" % d1_s[i[2]] , d0_s[i[1]], txt)\n",
    "                change+=1\n",
    "                attr_counter+=1\n",
    "                #substitutions[(d1_s[i[1]], d0_s[i[2]])] = substitutions.get((d1_s[i[1]], d0_s[i[2]]), 0) + 1 \n",
    "                substitutions[(d0_s[i[1]], d1_s[i[2]])] = substitutions.get((d0_s[i[1]], d1_s[i[2]]), 0) + 1\n",
    "\n",
    "       # update related variables accordingly\n",
    "        new_txt = ends_with_fullstop(txt)\n",
    "        all_swaps.append(new_txt)\n",
    "        if_change = check_if_changed(change, if_change)\n",
    "        \n",
    "        \n",
    "    return all_swaps, if_change, attr_counter, substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e80d7d4-16d7-4dd4-8fa7-bdace4af053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edits(sentences, pos, thresh=100, baseline=True, antonyms=False):\n",
    "    substitution_singular, d0_s, d1_s, g = None, None, None, None\n",
    "\n",
    "    # use appropriate function based on pos to get feasible substitutions\n",
    "    if pos == 'adv':\n",
    "        substitution_singular, d0_s, d1_s, g = graph_adverb_substitutions(sentences, baseline, antonyms=antonyms)\n",
    "    elif pos == 'verb':\n",
    "        substitution_singular, d0_s, d1_s, g = graph_verb_substitutions(sentences, baseline, antonyms=antonyms)\n",
    "    elif pos == 'noun':\n",
    "        substitution_singular, d0_s, d1_s, g = graph_noun_substitutions(sentences, baseline, antonyms=antonyms)\n",
    "    else:\n",
    "        raise AttributeError(\"pos '{}' is not supported!\".format(pos))\n",
    "\n",
    "    # return the edited sentences\n",
    "    return external_swaps(sentences, pos=pos, substitution_singular=substitution_singular, d0_s=d0_s, d1_s=d1_s, thresh=thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a58cf6-7f91-401d-8479-74e3612f44c6",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1172b1c6-03cb-4b39-9893-16b18f34c4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('G0_0', 'G1_1'), ('G0_1', 'G1_4'), ('G0_2', 'G1_3'), ('G0_3', 'G1_2'), ('G0_4', 'G1_0'), ('G1_1', 'G0_0'), ('G1_4', 'G0_1'), ('G1_3', 'G0_2'), ('G1_2', 'G0_3'), ('G1_0', 'G0_4')]\n",
      "\n",
      "[('G0_3', 'G1_2'), ('G0_4', 'G1_0'), ('G0_2', 'G1_3'), ('G0_0', 'G1_1'), ('G0_1', 'G1_4')]\n",
      "\n",
      "[(0.09090909090909091, Synset('small.n.01'), Synset('large.n.01')), (0.09090909090909091, Synset('tall.n.01'), Synset('great.n.01')), (0.09090909090909091, Synset('large.n.01'), Synset('small.n.01')), (0.09090909090909091, Synset('great.n.01'), Synset('fat.n.01')), (0.07692307692307693, Synset('fat.n.01'), Synset('tall.n.01'))]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[('G0_0', 'G1_6'), ('G0_8', 'G1_1'), ('G0_1', 'G1_5'), ('G0_2', 'G1_4'), ('G0_7', 'G1_8'), ('G0_3', 'G1_7'), ('G0_5', 'G1_3'), ('G0_4', 'G1_2'), ('G0_6', 'G1_0'), ('G1_6', 'G0_0'), ('G1_1', 'G0_8'), ('G1_5', 'G0_1'), ('G1_4', 'G0_2'), ('G1_8', 'G0_7'), ('G1_7', 'G0_3'), ('G1_3', 'G0_5'), ('G1_2', 'G0_4'), ('G1_0', 'G0_6')]\n",
      "\n",
      "[('G0_6', 'G1_0'), ('G0_4', 'G1_2'), ('G0_0', 'G1_6'), ('G0_8', 'G1_1'), ('G0_7', 'G1_8'), ('G0_2', 'G1_4'), ('G0_3', 'G1_7'), ('G0_1', 'G1_5'), ('G0_5', 'G1_3')]\n",
      "\n",
      "[(0.08333333333333333, Synset('town.n.01'), Synset('man.n.01')), (0.07142857142857142, Synset('building.n.01'), Synset('army.n.01')), (0.08333333333333333, Synset('man.n.01'), Synset('town.n.01')), (0.1111111111111111, Synset('hill.n.01'), Synset('male_child.n.01')), (0.1, Synset('dog.n.01'), Synset('hill.n.01')), (0.07142857142857142, Synset('army.n.01'), Synset('building.n.01')), (0.08333333333333333, Synset('river.n.01'), Synset('dog.n.01')), (0.1, Synset('male_child.n.01'), Synset('stick.n.01')), (0.09090909090909091, Synset('stick.n.01'), Synset('river.n.01'))]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[('G0_1', 'G1_0'), ('G0_0', 'G1_1'), ('G1_0', 'G0_1'), ('G1_1', 'G0_0')]\n",
      "\n",
      "[('G0_1', 'G1_0'), ('G0_0', 'G1_1')]\n",
      "\n",
      "[(0.09090909090909091, Synset('tax_return.n.01'), Synset('standing.n.01')), (0.09090909090909091, Synset('standing.n.01'), Synset('tax_return.n.01'))]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'sents': [\n",
    "        'A great man was standing in a tall and magnificent hill, gazing upon the sad and destructive army',\n",
    "        'The clever boy was wondering when the fat dog would return with the big stick',\n",
    "        'A small town was standing next to the large river and the tall building'\n",
    "    ]\n",
    "})\n",
    "\n",
    "for pos in ['adv', 'noun', 'verb']:\n",
    "    swaps, if_change, counter, substitutions = get_edits(df['sents'], pos=pos, thresh=4)\n",
    "#     print(\"{} SWAPS:\".format(pos.upper()))\n",
    "#     print(swaps)\n",
    "#     print()\n",
    "#     print(if_change)\n",
    "#     print()\n",
    "#     print(counter)\n",
    "#     print()\n",
    "#     print(substitutions)\n",
    "#     print(\"====================================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "051c325e-c9fe-4b0b-97b5-967a8e820a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "man = wn.synset('man.n.01')\n",
    "boy = wn.synset('boy.n.01')\n",
    "town = wn.synset('town.n.01')\n",
    "large = wn.synset('large.a.01')\n",
    "great = wn.synset('great.a.01')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
